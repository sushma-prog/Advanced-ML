{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "13aba6b2-dd98-4643-9300-85fd4ad110ee",
   "metadata": {},
   "source": [
    "\n",
    "## üî∑ **Step 1: Understand Ensemble Learning ‚Äì Bagging vs Boosting**\n",
    "\n",
    "---\n",
    " \n",
    "### üí° What is Ensemble Learning?\n",
    "\n",
    "> **Ensemble Learning** is a technique where **multiple models (learners)** are combined to solve a problem ‚Äî usually for **better accuracy** than any single model.\n",
    "\n",
    "Think of it like:\n",
    "\n",
    "> *Asking 10 average students the same question and taking a majority vote* ‚Üí Better than trusting 1 student!\n",
    "\n",
    "There are **two main types**:\n",
    "\n",
    "‚úÖ **Bagging** (like Random Forest)\n",
    "\n",
    "‚úÖ **Boosting** (like XGBoost, LightGBM)\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What is Bagging?\n",
    "\n",
    "**Bagging = Bootstrap Aggregating**\n",
    "\n",
    "### üß† Key Ideas:\n",
    "\n",
    "* Trains multiple models **independently** on **random subsets** of data (with replacement).\n",
    " \n",
    "* Final result is based on **majority vote** (for classification) or **average** (for regression).\n",
    "  \n",
    "* Reduces **variance** (helps avoid overfitting).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example: Random Forest\n",
    "\n",
    "* Creates **many decision trees**.\n",
    "  \n",
    "* Each tree sees a **random portion** of data and features.\n",
    "  \n",
    "* The final prediction is the **average** of all trees' outputs.\n",
    "\n",
    "> üî∏ Benefit: Even if some trees are wrong, the group decision is stable.\n",
    "\n",
    "> üî∏ Weakness: Not great if features are weak or all trees make similar mistakes.\n",
    "\n",
    "---\n",
    "\n",
    "## üîπ What is Boosting?\n",
    "\n",
    "**Boosting = Sequential Learning**\n",
    "\n",
    "### üß† Key Ideas:\n",
    "\n",
    "* Models are built **one after another**.\n",
    "  \n",
    "* Each new model focuses on the **errors** made by the previous model.\n",
    "\n",
    "* It learns which data points were misclassified or had high error.\n",
    "\n",
    "* Gradually improves performance.\n",
    "\n",
    "* Reduces **bias** (learns complex patterns better).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Example: Gradient Boosting, XGBoost, LightGBM\n",
    "\n",
    "* First model learns the data.\n",
    "\n",
    "* Second model learns to correct the mistakes.\n",
    "\n",
    "* Third model learns to fix second‚Äôs mistakes... and so on.\n",
    "\n",
    "* Final result = sum of all models' predictions.\n",
    "\n",
    "> üî∏ Benefit: Very powerful! Often wins machine learning competitions.\n",
    "\n",
    "> üî∏ Weakness: Can **overfit** if not tuned carefully, and **slower** than bagging.\n",
    "\n",
    "---\n",
    "\n",
    "### üß™ Difference Table:\n",
    "\n",
    "| Feature             | Bagging                 | Boosting                             |\n",
    "| ------------------- | ----------------------- | ------------------------------------ |\n",
    "| Model Training      | **Parallel**            | **Sequential**                       |\n",
    "| Goal                | Reduce **variance**     | Reduce **bias**                      |\n",
    "| Example Algorithm   | Random Forest           | Gradient Boosting, XGBoost           |\n",
    "| Handles Overfitting | Well (better stability) | Risk of overfitting (tune carefully) |\n",
    "| Speed               | Faster                  | Slower                               |\n",
    "\n",
    "---\n",
    "\n",
    "### üéØ Simple Analogy:\n",
    "\n",
    "|                                                     | Bagging            | Boosting                                                              |                     |\n",
    "| --------------------------------------------------- | ------------------ | --------------------------------------------------------------------- | ------------------- |\n",
    "| Imagine 10 doctors independently give their opinion | That‚Äôs **Bagging** | First doctor gives opinion, second improves it, third builds on that‚Ä¶ | That‚Äôs **Boosting** |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary of Step 1:\n",
    "\n",
    "* **Ensemble learning** boosts accuracy by combining models.\n",
    "\n",
    "* **Bagging** builds independent models ‚Üí reduces **variance** (overfitting).\n",
    "\n",
    "* **Boosting** builds sequential models ‚Üí reduces **bias** (underfitting).\n",
    "\n",
    "* Popular boosting methods: **Gradient Boosting**, **XGBoost**, **LightGBM**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe0c227a-b2d3-4d39-b47c-81105ff7886f",
   "metadata": {},
   "source": [
    "## üî∑ **Step 2: Learn Gradient Boosting Basics**\n",
    "\n",
    "---\n",
    "\n",
    "### üí° What is Gradient Boosting?\n",
    "\n",
    "Gradient Boosting is a **machine learning technique** that:\n",
    "\n",
    "* Builds models **sequentially** (one after another).\n",
    "\n",
    "* Each new model **corrects the mistakes** made by the previous model.\n",
    "\n",
    "* The final prediction is a **weighted sum** of all the previous models.\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Why \"Gradient\" in Gradient Boosting?\n",
    "\n",
    "Because it uses **gradient descent** to **minimize the loss (error)** step by step.\n",
    "\n",
    "Imagine this:\n",
    "\n",
    "> You start with a bad model, then slowly take steps (gradients) to fix its errors until you reach a better model.\n",
    "\n",
    "---\n",
    "\n",
    "### üìà Step-by-Step Intuition:\n",
    "\n",
    "Let's say we want to **predict house prices**.\n",
    "\n",
    "1. **Model 1** predicts:\n",
    "\n",
    "   ‚Üí But it's not accurate (lots of error).\n",
    "\n",
    "2. **Model 2** is trained on the **errors (residuals)** of Model 1.\n",
    "\n",
    "   ‚Üí So now it learns *where Model 1 was wrong*.\n",
    "\n",
    "3. **Model 3** is trained on the updated errors.\n",
    " \n",
    "   ‚Üí Keeps improving the predictions.\n",
    "\n",
    "üßÆ Final Prediction =\n",
    "`Model1 output + Model2 corrections + Model3 corrections + ...`\n",
    "\n",
    "---\n",
    "\n",
    "### ‚öôÔ∏è What's Happening Under the Hood?\n",
    "\n",
    "1. **Start with a weak model** (e.g., a small decision tree).\n",
    "\n",
    "2. **Calculate residuals (errors)**:\n",
    "   Actual - Predicted = Error\n",
    "\n",
    "3. **Fit a new model** to those residuals.\n",
    "\n",
    "4. **Repeat** steps 2‚Äì3 for many rounds.\n",
    "\n",
    "5. Add up the predictions of all models.\n",
    "\n",
    "This process **minimizes the loss function** using **gradient descent**.\n",
    "\n",
    "---\n",
    "\n",
    "### üîç What‚Äôs a ‚ÄúWeak Learner‚Äù?\n",
    "\n",
    "A weak learner is a model that does **slightly better than random guessing**.\n",
    "\n",
    "In Gradient Boosting, we typically use **shallow decision trees** (e.g., depth = 3).\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Common Loss Functions:\n",
    "\n",
    "| Problem Type               | Loss Function                   |\n",
    "| -------------------------- | ------------------------------- |\n",
    "| Regression                 | Mean Squared Error              |\n",
    "| Binary Classification      | Log Loss (Binary Cross-Entropy) |\n",
    "| Multi-class Classification | Multiclass Log Loss             |\n",
    "\n",
    "The algorithm **computes the gradient of the loss** and uses it to guide the next tree.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö° Why It Works So Well:\n",
    "\n",
    "‚úÖ Focuses on difficult examples (those with higher errors)\n",
    "\n",
    "‚úÖ Gradually improves predictions\n",
    "\n",
    "‚úÖ Can model **non-linear relationships**\n",
    "\n",
    "‚úÖ Works well with **both numerical and categorical** data (after encoding)\n",
    "\n",
    "---\n",
    "\n",
    "### üö® But... It‚Äôs Not Perfect:\n",
    "\n",
    "| Issue                 | Why it matters                                                           |\n",
    "| --------------------- | ------------------------------------------------------------------------ |\n",
    "| Overfitting risk      | Too many rounds = model memorizes data                                   |\n",
    "| Slow training         | Especially with large datasets                                           |\n",
    "| Sensitive to outliers | Because error terms are squared in many losses                           |\n",
    "| Needs tuning          | Parameters like `n_estimators`, `learning_rate` affect performance a lot |\n",
    "\n",
    "---\n",
    "\n",
    "### üß† Summary of Step 2:\n",
    "\n",
    "* Gradient Boosting builds models **sequentially** to **reduce error**.\n",
    "\n",
    "* Each new model corrects the previous one‚Äôs mistakes.\n",
    "\n",
    "* Uses **gradient descent** to minimize a **loss function**.\n",
    "\n",
    "* Final prediction is a **sum** of all models‚Äô outputs.\n",
    "\n",
    "* Powerful but requires **tuning and care**.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d10f834a-1fdb-4d2a-9dd6-4dce9d20993f",
   "metadata": {},
   "source": [
    "## üî∑ **Step 3: Understand XGBoost and LightGBM**\n",
    "\n",
    "These are **advanced implementations of Gradient Boosting** ‚Äî optimized for **speed, performance, and scalability**.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 1. **XGBoost** (Extreme Gradient Boosting)\n",
    "\n",
    "#### üîπ What is it?\n",
    "\n",
    "A **high-performance version of gradient boosting**, developed by Tianqi Chen. It‚Äôs widely used in Kaggle competitions and industry.\n",
    "\n",
    "---\n",
    "\n",
    "#### üöÄ Why it's popular:\n",
    "\n",
    "| Feature                       | What it means                                               |\n",
    "| ----------------------------- | ----------------------------------------------------------- |\n",
    "| **Regularization**            | Helps avoid **overfitting** (like L1, L2 penalties).        |\n",
    "| **Parallel processing**       | Speeds up training by using multiple CPU cores.             |\n",
    "| **Handling missing values**   | Automatically learns best direction for missing data.       |\n",
    "| **Tree pruning**              | Uses a smart pruning strategy to reduce unnecessary splits. |\n",
    "| **Custom loss functions**     | Can define your own error metric.                           |\n",
    "| **Built-in cross-validation** | Easier model tuning with less manual coding.                |\n",
    "\n",
    "---\n",
    "\n",
    "#### üîß Parameters to Know:\n",
    "\n",
    "| Parameter          | What it controls                              |\n",
    "| ------------------ | --------------------------------------------- |\n",
    "| `n_estimators`     | Number of trees                               |\n",
    "| `learning_rate`    | Step size for each tree's correction          |\n",
    "| `max_depth`        | Maximum depth of a tree                       |\n",
    "| `subsample`        | % of data used per tree (reduces overfitting) |\n",
    "| `colsample_bytree` | % of features used per tree                   |\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ 2. **LightGBM** (Light Gradient Boosting Machine)\n",
    "\n",
    "#### üîπ What is it?\n",
    "\n",
    "A newer, **faster, more memory-efficient** gradient boosting library developed by Microsoft.\n",
    "\n",
    "---\n",
    "\n",
    "#### ‚ö° Key Differences from XGBoost:\n",
    "\n",
    "| Feature                              | LightGBM                                                      | XGBoost                            |\n",
    "| ------------------------------------ | ------------------------------------------------------------- | ---------------------------------- |\n",
    "| **Tree Growth**                      | Grows **leaf-wise** ‚Üí better accuracy but risk of overfitting | Grows **level-wise** ‚Üí more stable |\n",
    "| **Speed**                            | Usually **faster** on large datasets                          | Slower for very large data         |\n",
    "| **Memory**                           | Uses less memory                                              | More memory usage                  |\n",
    "| **Accuracy**                         | Often slightly better (but depends on tuning)                 | Very reliable and stable           |\n",
    "| **Handling of Categorical Features** | Can use raw categorical data directly                         | Needs label encoding or one-hot    |\n",
    "\n",
    "---\n",
    "\n",
    "#### üß† What does **leaf-wise** vs **level-wise** mean?\n",
    "\n",
    "Let‚Äôs say you grow trees:\n",
    "\n",
    "* **XGBoost (level-wise)**:\n",
    " \n",
    "    Grows all branches of the tree **equally**, one level at a time ‚Üí Balanced growth.\n",
    "\n",
    "* **LightGBM (leaf-wise)**:\n",
    " \n",
    "    Grows the **most important leaf** (with highest loss) ‚Üí More focused learning, faster convergence.\n",
    "\n",
    "‚ö†Ô∏è But leaf-wise can **overfit quickly** if not regularized properly.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Summary ‚Äì When to Use What?\n",
    "\n",
    "| Use Case                              | Recommended |\n",
    "| ------------------------------------- | ----------- |\n",
    "| You want **stability + robustness**   | XGBoost     |\n",
    "| You want **speed + large dataset**    | LightGBM    |\n",
    "| You‚Äôre doing a **Kaggle competition** | Try both!   |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44505476-1c65-42e2-9e8f-39acc86d40c1",
   "metadata": {},
   "source": [
    "## üî∑ **Step 4: Install the Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b7949d0-5dcc-4305-a7ef-4cd1bff21bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.2-py3-none-win_amd64.whl.metadata (2.1 kB)\n",
      "Collecting lightgbm\n",
      "  Downloading lightgbm-4.6.0-py3-none-win_amd64.whl.metadata (17 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\lucky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (2.0.0)\n",
      "Requirement already satisfied: scipy in c:\\users\\lucky\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xgboost) (1.15.1)\n",
      "Downloading xgboost-3.0.2-py3-none-win_amd64.whl (150.0 MB)\n",
      "   ---------------------------------------- 0.0/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/150.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.5/150.0 MB 1.5 MB/s eta 0:01:39\n",
      "   ---------------------------------------- 0.8/150.0 MB 1.5 MB/s eta 0:01:43\n",
      "   ---------------------------------------- 1.3/150.0 MB 1.7 MB/s eta 0:01:27\n",
      "   ---------------------------------------- 1.6/150.0 MB 1.7 MB/s eta 0:01:25\n",
      "    --------------------------------------- 2.4/150.0 MB 2.0 MB/s eta 0:01:15\n",
      "    --------------------------------------- 2.9/150.0 MB 2.0 MB/s eta 0:01:12\n",
      "    --------------------------------------- 3.1/150.0 MB 2.1 MB/s eta 0:01:12\n",
      "    --------------------------------------- 3.4/150.0 MB 2.0 MB/s eta 0:01:14\n",
      "    --------------------------------------- 3.7/150.0 MB 1.9 MB/s eta 0:01:17\n",
      "   - -------------------------------------- 4.2/150.0 MB 1.9 MB/s eta 0:01:18\n",
      "   - -------------------------------------- 4.5/150.0 MB 1.9 MB/s eta 0:01:19\n",
      "   - -------------------------------------- 4.7/150.0 MB 1.9 MB/s eta 0:01:19\n",
      "   - -------------------------------------- 5.0/150.0 MB 1.8 MB/s eta 0:01:20\n",
      "   - -------------------------------------- 5.5/150.0 MB 1.8 MB/s eta 0:01:21\n",
      "   - -------------------------------------- 5.8/150.0 MB 1.8 MB/s eta 0:01:21\n",
      "   - -------------------------------------- 6.0/150.0 MB 1.8 MB/s eta 0:01:21\n",
      "   - -------------------------------------- 6.6/150.0 MB 1.8 MB/s eta 0:01:22\n",
      "   - -------------------------------------- 6.8/150.0 MB 1.7 MB/s eta 0:01:23\n",
      "   - -------------------------------------- 7.1/150.0 MB 1.7 MB/s eta 0:01:23\n",
      "   - -------------------------------------- 7.3/150.0 MB 1.7 MB/s eta 0:01:24\n",
      "   -- ------------------------------------- 7.9/150.0 MB 1.7 MB/s eta 0:01:25\n",
      "   -- ------------------------------------- 8.1/150.0 MB 1.7 MB/s eta 0:01:24\n",
      "   -- ------------------------------------- 8.1/150.0 MB 1.7 MB/s eta 0:01:24\n",
      "   -- ------------------------------------- 8.4/150.0 MB 1.6 MB/s eta 0:01:28\n",
      "   -- ------------------------------------- 8.7/150.0 MB 1.6 MB/s eta 0:01:31\n",
      "   -- ------------------------------------- 8.7/150.0 MB 1.6 MB/s eta 0:01:31\n",
      "   -- ------------------------------------- 8.9/150.0 MB 1.5 MB/s eta 0:01:34\n",
      "   -- ------------------------------------- 8.9/150.0 MB 1.5 MB/s eta 0:01:34\n",
      "   -- ------------------------------------- 9.2/150.0 MB 1.5 MB/s eta 0:01:37\n",
      "   -- ------------------------------------- 9.4/150.0 MB 1.4 MB/s eta 0:01:39\n",
      "   -- ------------------------------------- 9.7/150.0 MB 1.4 MB/s eta 0:01:39\n",
      "   -- ------------------------------------- 9.7/150.0 MB 1.4 MB/s eta 0:01:39\n",
      "   -- ------------------------------------- 10.2/150.0 MB 1.4 MB/s eta 0:01:39\n",
      "   -- ------------------------------------- 10.5/150.0 MB 1.4 MB/s eta 0:01:39\n",
      "   -- ------------------------------------- 10.7/150.0 MB 1.4 MB/s eta 0:01:39\n",
      "   -- ------------------------------------- 11.0/150.0 MB 1.4 MB/s eta 0:01:38\n",
      "   --- ------------------------------------ 11.5/150.0 MB 1.4 MB/s eta 0:01:38\n",
      "   --- ------------------------------------ 11.8/150.0 MB 1.4 MB/s eta 0:01:38\n",
      "   --- ------------------------------------ 12.1/150.0 MB 1.4 MB/s eta 0:01:38\n",
      "   --- ------------------------------------ 12.3/150.0 MB 1.4 MB/s eta 0:01:38\n",
      "   --- ------------------------------------ 12.3/150.0 MB 1.4 MB/s eta 0:01:38\n",
      "   --- ------------------------------------ 12.6/150.0 MB 1.4 MB/s eta 0:01:39\n",
      "   --- ------------------------------------ 13.1/150.0 MB 1.4 MB/s eta 0:01:39\n",
      "   --- ------------------------------------ 13.6/150.0 MB 1.4 MB/s eta 0:01:37\n",
      "   --- ------------------------------------ 13.9/150.0 MB 1.4 MB/s eta 0:01:36\n",
      "   --- ------------------------------------ 14.2/150.0 MB 1.4 MB/s eta 0:01:36\n",
      "   --- ------------------------------------ 14.4/150.0 MB 1.4 MB/s eta 0:01:36\n",
      "   --- ------------------------------------ 14.7/150.0 MB 1.4 MB/s eta 0:01:36\n",
      "   ---- ----------------------------------- 15.2/150.0 MB 1.4 MB/s eta 0:01:36\n",
      "   ---- ----------------------------------- 15.5/150.0 MB 1.4 MB/s eta 0:01:35\n",
      "   ---- ----------------------------------- 16.0/150.0 MB 1.4 MB/s eta 0:01:34\n",
      "   ---- ----------------------------------- 16.5/150.0 MB 1.5 MB/s eta 0:01:32\n",
      "   ---- ----------------------------------- 17.0/150.0 MB 1.5 MB/s eta 0:01:31\n",
      "   ---- ----------------------------------- 17.6/150.0 MB 1.5 MB/s eta 0:01:29\n",
      "   ---- ----------------------------------- 18.1/150.0 MB 1.5 MB/s eta 0:01:28\n",
      "   ---- ----------------------------------- 18.4/150.0 MB 1.5 MB/s eta 0:01:28\n",
      "   ----- ---------------------------------- 18.9/150.0 MB 1.5 MB/s eta 0:01:27\n",
      "   ----- ---------------------------------- 19.4/150.0 MB 1.5 MB/s eta 0:01:26\n",
      "   ----- ---------------------------------- 20.2/150.0 MB 1.6 MB/s eta 0:01:24\n",
      "   ----- ---------------------------------- 20.7/150.0 MB 1.6 MB/s eta 0:01:23\n",
      "   ----- ---------------------------------- 21.0/150.0 MB 1.6 MB/s eta 0:01:22\n",
      "   ----- ---------------------------------- 21.5/150.0 MB 1.6 MB/s eta 0:01:21\n",
      "   ----- ---------------------------------- 22.3/150.0 MB 1.6 MB/s eta 0:01:20\n",
      "   ------ --------------------------------- 22.5/150.0 MB 1.6 MB/s eta 0:01:19\n",
      "   ------ --------------------------------- 23.1/150.0 MB 1.6 MB/s eta 0:01:19\n",
      "   ------ --------------------------------- 23.6/150.0 MB 1.6 MB/s eta 0:01:18\n",
      "   ------ --------------------------------- 23.9/150.0 MB 1.6 MB/s eta 0:01:18\n",
      "   ------ --------------------------------- 23.9/150.0 MB 1.6 MB/s eta 0:01:18\n",
      "   ------ --------------------------------- 24.4/150.0 MB 1.6 MB/s eta 0:01:18\n",
      "   ------ --------------------------------- 24.9/150.0 MB 1.6 MB/s eta 0:01:17\n",
      "   ------ --------------------------------- 25.2/150.0 MB 1.6 MB/s eta 0:01:17\n",
      "   ------ --------------------------------- 26.0/150.0 MB 1.7 MB/s eta 0:01:15\n",
      "   ------ --------------------------------- 26.2/150.0 MB 1.7 MB/s eta 0:01:15\n",
      "   ------- -------------------------------- 26.7/150.0 MB 1.7 MB/s eta 0:01:15\n",
      "   ------- -------------------------------- 27.5/150.0 MB 1.7 MB/s eta 0:01:13\n",
      "   ------- -------------------------------- 28.0/150.0 MB 1.7 MB/s eta 0:01:12\n",
      "   ------- -------------------------------- 28.3/150.0 MB 1.7 MB/s eta 0:01:12\n",
      "   ------- -------------------------------- 28.8/150.0 MB 1.7 MB/s eta 0:01:12\n",
      "   ------- -------------------------------- 29.4/150.0 MB 1.7 MB/s eta 0:01:11\n",
      "   ------- -------------------------------- 29.9/150.0 MB 1.7 MB/s eta 0:01:10\n",
      "   -------- ------------------------------- 30.4/150.0 MB 1.7 MB/s eta 0:01:10\n",
      "   -------- ------------------------------- 30.7/150.0 MB 1.7 MB/s eta 0:01:10\n",
      "   -------- ------------------------------- 30.9/150.0 MB 1.7 MB/s eta 0:01:10\n",
      "   -------- ------------------------------- 31.5/150.0 MB 1.7 MB/s eta 0:01:09\n",
      "   -------- ------------------------------- 32.0/150.0 MB 1.7 MB/s eta 0:01:09\n",
      "   -------- ------------------------------- 32.5/150.0 MB 1.7 MB/s eta 0:01:08\n",
      "   -------- ------------------------------- 33.0/150.0 MB 1.7 MB/s eta 0:01:08\n",
      "   -------- ------------------------------- 33.6/150.0 MB 1.8 MB/s eta 0:01:07\n",
      "   --------- ------------------------------ 34.1/150.0 MB 1.8 MB/s eta 0:01:06\n",
      "   --------- ------------------------------ 34.6/150.0 MB 1.8 MB/s eta 0:01:06\n",
      "   --------- ------------------------------ 34.9/150.0 MB 1.8 MB/s eta 0:01:05\n",
      "   --------- ------------------------------ 35.7/150.0 MB 1.8 MB/s eta 0:01:05\n",
      "   --------- ------------------------------ 36.2/150.0 MB 1.8 MB/s eta 0:01:04\n",
      "   --------- ------------------------------ 36.7/150.0 MB 1.8 MB/s eta 0:01:04\n",
      "   --------- ------------------------------ 37.5/150.0 MB 1.8 MB/s eta 0:01:03\n",
      "   ---------- ----------------------------- 37.7/150.0 MB 1.8 MB/s eta 0:01:02\n",
      "   ---------- ----------------------------- 38.5/150.0 MB 1.8 MB/s eta 0:01:02\n",
      "   ---------- ----------------------------- 39.1/150.0 MB 1.8 MB/s eta 0:01:01\n",
      "   ---------- ----------------------------- 39.6/150.0 MB 1.8 MB/s eta 0:01:01\n",
      "   ---------- ----------------------------- 40.1/150.0 MB 1.8 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 40.6/150.0 MB 1.8 MB/s eta 0:01:00\n",
      "   ---------- ----------------------------- 40.9/150.0 MB 1.9 MB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 41.4/150.0 MB 1.9 MB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 41.9/150.0 MB 1.9 MB/s eta 0:00:59\n",
      "   ----------- ---------------------------- 42.5/150.0 MB 1.9 MB/s eta 0:00:58\n",
      "   ----------- ---------------------------- 43.0/150.0 MB 1.9 MB/s eta 0:00:58\n",
      "   ----------- ---------------------------- 43.8/150.0 MB 1.9 MB/s eta 0:00:57\n",
      "   ----------- ---------------------------- 44.3/150.0 MB 1.9 MB/s eta 0:00:56\n",
      "   ------------ --------------------------- 45.1/150.0 MB 1.9 MB/s eta 0:00:55\n",
      "   ------------ --------------------------- 45.6/150.0 MB 1.9 MB/s eta 0:00:55\n",
      "   ------------ --------------------------- 46.1/150.0 MB 1.9 MB/s eta 0:00:55\n",
      "   ------------ --------------------------- 46.9/150.0 MB 1.9 MB/s eta 0:00:54\n",
      "   ------------ --------------------------- 47.4/150.0 MB 1.9 MB/s eta 0:00:53\n",
      "   ------------ --------------------------- 48.2/150.0 MB 1.9 MB/s eta 0:00:53\n",
      "   ------------- -------------------------- 49.0/150.0 MB 2.0 MB/s eta 0:00:52\n",
      "   ------------- -------------------------- 49.8/150.0 MB 2.0 MB/s eta 0:00:51\n",
      "   ------------- -------------------------- 50.3/150.0 MB 2.0 MB/s eta 0:00:51\n",
      "   ------------- -------------------------- 50.9/150.0 MB 2.0 MB/s eta 0:00:50\n",
      "   ------------- -------------------------- 51.1/150.0 MB 2.0 MB/s eta 0:00:50\n",
      "   ------------- -------------------------- 51.4/150.0 MB 2.0 MB/s eta 0:00:50\n",
      "   ------------- -------------------------- 51.9/150.0 MB 2.0 MB/s eta 0:00:50\n",
      "   ------------- -------------------------- 52.2/150.0 MB 2.0 MB/s eta 0:00:50\n",
      "   ------------- -------------------------- 52.4/150.0 MB 2.0 MB/s eta 0:00:50\n",
      "   -------------- ------------------------- 53.0/150.0 MB 2.0 MB/s eta 0:00:50\n",
      "   -------------- ------------------------- 53.5/150.0 MB 2.0 MB/s eta 0:00:49\n",
      "   -------------- ------------------------- 54.0/150.0 MB 2.0 MB/s eta 0:00:49\n",
      "   -------------- ------------------------- 54.5/150.0 MB 2.0 MB/s eta 0:00:49\n",
      "   -------------- ------------------------- 55.1/150.0 MB 2.0 MB/s eta 0:00:48\n",
      "   -------------- ------------------------- 55.6/150.0 MB 2.0 MB/s eta 0:00:48\n",
      "   --------------- ------------------------ 56.4/150.0 MB 2.0 MB/s eta 0:00:47\n",
      "   --------------- ------------------------ 56.9/150.0 MB 2.0 MB/s eta 0:00:47\n",
      "   --------------- ------------------------ 57.7/150.0 MB 2.0 MB/s eta 0:00:46\n",
      "   --------------- ------------------------ 58.2/150.0 MB 2.0 MB/s eta 0:00:46\n",
      "   --------------- ------------------------ 58.7/150.0 MB 2.0 MB/s eta 0:00:46\n",
      "   --------------- ------------------------ 59.2/150.0 MB 2.0 MB/s eta 0:00:45\n",
      "   ---------------- ----------------------- 60.0/150.0 MB 2.0 MB/s eta 0:00:45\n",
      "   ---------------- ----------------------- 60.8/150.0 MB 2.0 MB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 61.3/150.0 MB 2.0 MB/s eta 0:00:44\n",
      "   ---------------- ----------------------- 61.9/150.0 MB 2.1 MB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 62.1/150.0 MB 2.1 MB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 62.7/150.0 MB 2.1 MB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 63.2/150.0 MB 2.1 MB/s eta 0:00:43\n",
      "   ---------------- ----------------------- 63.7/150.0 MB 2.1 MB/s eta 0:00:42\n",
      "   ----------------- ---------------------- 64.2/150.0 MB 2.1 MB/s eta 0:00:42\n",
      "   ----------------- ---------------------- 65.0/150.0 MB 2.1 MB/s eta 0:00:42\n",
      "   ----------------- ---------------------- 65.5/150.0 MB 2.1 MB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 66.1/150.0 MB 2.1 MB/s eta 0:00:41\n",
      "   ----------------- ---------------------- 66.8/150.0 MB 2.1 MB/s eta 0:00:40\n",
      "   ----------------- ---------------------- 67.4/150.0 MB 2.1 MB/s eta 0:00:40\n",
      "   ------------------ --------------------- 68.2/150.0 MB 2.1 MB/s eta 0:00:39\n",
      "   ------------------ --------------------- 68.7/150.0 MB 2.1 MB/s eta 0:00:39\n",
      "   ------------------ --------------------- 69.5/150.0 MB 2.1 MB/s eta 0:00:38\n",
      "   ------------------ --------------------- 70.3/150.0 MB 2.2 MB/s eta 0:00:38\n",
      "   ------------------ --------------------- 70.5/150.0 MB 2.2 MB/s eta 0:00:37\n",
      "   ------------------ --------------------- 71.0/150.0 MB 2.2 MB/s eta 0:00:37\n",
      "   ------------------- -------------------- 71.3/150.0 MB 2.2 MB/s eta 0:00:37\n",
      "   ------------------- -------------------- 72.1/150.0 MB 2.2 MB/s eta 0:00:36\n",
      "   ------------------- -------------------- 72.9/150.0 MB 2.2 MB/s eta 0:00:36\n",
      "   ------------------- -------------------- 73.7/150.0 MB 2.2 MB/s eta 0:00:35\n",
      "   ------------------- -------------------- 74.2/150.0 MB 2.2 MB/s eta 0:00:35\n",
      "   ------------------- -------------------- 75.0/150.0 MB 2.2 MB/s eta 0:00:34\n",
      "   -------------------- ------------------- 75.5/150.0 MB 2.2 MB/s eta 0:00:34\n",
      "   -------------------- ------------------- 76.3/150.0 MB 2.3 MB/s eta 0:00:33\n",
      "   -------------------- ------------------- 76.8/150.0 MB 2.3 MB/s eta 0:00:32\n",
      "   -------------------- ------------------- 77.1/150.0 MB 2.3 MB/s eta 0:00:32\n",
      "   -------------------- ------------------- 77.6/150.0 MB 2.3 MB/s eta 0:00:32\n",
      "   -------------------- ------------------- 78.1/150.0 MB 2.3 MB/s eta 0:00:32\n",
      "   -------------------- ------------------- 78.6/150.0 MB 2.3 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 79.2/150.0 MB 2.3 MB/s eta 0:00:31\n",
      "   --------------------- ------------------ 80.0/150.0 MB 2.3 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 80.7/150.0 MB 2.4 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 81.3/150.0 MB 2.4 MB/s eta 0:00:30\n",
      "   --------------------- ------------------ 82.1/150.0 MB 2.4 MB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 82.6/150.0 MB 2.4 MB/s eta 0:00:29\n",
      "   ---------------------- ----------------- 83.1/150.0 MB 2.4 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 83.9/150.0 MB 2.4 MB/s eta 0:00:28\n",
      "   ---------------------- ----------------- 84.9/150.0 MB 2.4 MB/s eta 0:00:27\n",
      "   ---------------------- ----------------- 85.7/150.0 MB 2.5 MB/s eta 0:00:27\n",
      "   ---------------------- ----------------- 85.7/150.0 MB 2.5 MB/s eta 0:00:27\n",
      "   ----------------------- ---------------- 86.8/150.0 MB 2.5 MB/s eta 0:00:26\n",
      "   ----------------------- ---------------- 87.3/150.0 MB 2.5 MB/s eta 0:00:26\n",
      "   ----------------------- ---------------- 87.6/150.0 MB 2.5 MB/s eta 0:00:26\n",
      "   ----------------------- ---------------- 88.3/150.0 MB 2.5 MB/s eta 0:00:25\n",
      "   ----------------------- ---------------- 88.9/150.0 MB 2.5 MB/s eta 0:00:25\n",
      "   ----------------------- ---------------- 89.7/150.0 MB 2.5 MB/s eta 0:00:25\n",
      "   ------------------------ --------------- 90.2/150.0 MB 2.5 MB/s eta 0:00:24\n",
      "   ------------------------ --------------- 90.7/150.0 MB 2.5 MB/s eta 0:00:24\n",
      "   ------------------------ --------------- 91.5/150.0 MB 2.5 MB/s eta 0:00:24\n",
      "   ------------------------ --------------- 91.8/150.0 MB 2.5 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 92.3/150.0 MB 2.5 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 92.8/150.0 MB 2.5 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 93.1/150.0 MB 2.5 MB/s eta 0:00:23\n",
      "   ------------------------ --------------- 93.3/150.0 MB 2.5 MB/s eta 0:00:23\n",
      "   ------------------------- -------------- 93.8/150.0 MB 2.5 MB/s eta 0:00:23\n",
      "   ------------------------- -------------- 94.1/150.0 MB 2.5 MB/s eta 0:00:23\n",
      "   ------------------------- -------------- 94.6/150.0 MB 2.5 MB/s eta 0:00:22\n",
      "   ------------------------- -------------- 94.9/150.0 MB 2.5 MB/s eta 0:00:22\n",
      "   ------------------------- -------------- 95.7/150.0 MB 2.5 MB/s eta 0:00:22\n",
      "   ------------------------- -------------- 95.9/150.0 MB 2.5 MB/s eta 0:00:22\n",
      "   ------------------------- -------------- 96.5/150.0 MB 2.5 MB/s eta 0:00:22\n",
      "   ------------------------- -------------- 97.3/150.0 MB 2.5 MB/s eta 0:00:22\n",
      "   -------------------------- ------------- 97.8/150.0 MB 2.5 MB/s eta 0:00:21\n",
      "   -------------------------- ------------- 98.3/150.0 MB 2.5 MB/s eta 0:00:21\n",
      "   -------------------------- ------------- 98.8/150.0 MB 2.5 MB/s eta 0:00:21\n",
      "   -------------------------- ------------- 99.1/150.0 MB 2.5 MB/s eta 0:00:21\n",
      "   -------------------------- ------------- 100.1/150.0 MB 2.5 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 100.7/150.0 MB 2.5 MB/s eta 0:00:20\n",
      "   -------------------------- ------------- 101.2/150.0 MB 2.5 MB/s eta 0:00:20\n",
      "   --------------------------- ------------ 101.7/150.0 MB 2.5 MB/s eta 0:00:19\n",
      "   --------------------------- ------------ 102.5/150.0 MB 2.6 MB/s eta 0:00:19\n",
      "   --------------------------- ------------ 103.0/150.0 MB 2.6 MB/s eta 0:00:19\n",
      "   --------------------------- ------------ 103.5/150.0 MB 2.6 MB/s eta 0:00:19\n",
      "   --------------------------- ------------ 104.1/150.0 MB 2.6 MB/s eta 0:00:18\n",
      "   --------------------------- ------------ 104.6/150.0 MB 2.6 MB/s eta 0:00:18\n",
      "   ---------------------------- ----------- 105.1/150.0 MB 2.6 MB/s eta 0:00:18\n",
      "   ---------------------------- ----------- 105.6/150.0 MB 2.6 MB/s eta 0:00:18\n",
      "   ---------------------------- ----------- 106.2/150.0 MB 2.6 MB/s eta 0:00:18\n",
      "   ---------------------------- ----------- 106.7/150.0 MB 2.6 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 107.2/150.0 MB 2.6 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 107.7/150.0 MB 2.6 MB/s eta 0:00:17\n",
      "   ---------------------------- ----------- 108.3/150.0 MB 2.6 MB/s eta 0:00:17\n",
      "   ----------------------------- ---------- 108.8/150.0 MB 2.6 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 109.1/150.0 MB 2.6 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 109.6/150.0 MB 2.6 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 110.1/150.0 MB 2.6 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 110.6/150.0 MB 2.6 MB/s eta 0:00:16\n",
      "   ----------------------------- ---------- 111.4/150.0 MB 2.6 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 111.9/150.0 MB 2.6 MB/s eta 0:00:15\n",
      "   ----------------------------- ---------- 112.5/150.0 MB 2.6 MB/s eta 0:00:15\n",
      "   ------------------------------ --------- 113.2/150.0 MB 2.6 MB/s eta 0:00:15\n",
      "   ------------------------------ --------- 114.0/150.0 MB 2.6 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 114.6/150.0 MB 2.6 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 114.8/150.0 MB 2.6 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 115.3/150.0 MB 2.6 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 115.6/150.0 MB 2.6 MB/s eta 0:00:14\n",
      "   ------------------------------ --------- 116.1/150.0 MB 2.6 MB/s eta 0:00:14\n",
      "   ------------------------------- -------- 116.7/150.0 MB 2.6 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 117.2/150.0 MB 2.6 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 118.0/150.0 MB 2.6 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 118.2/150.0 MB 2.6 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 118.5/150.0 MB 2.6 MB/s eta 0:00:13\n",
      "   ------------------------------- -------- 119.3/150.0 MB 2.6 MB/s eta 0:00:12\n",
      "   ------------------------------- -------- 119.8/150.0 MB 2.6 MB/s eta 0:00:12\n",
      "   -------------------------------- ------- 120.6/150.0 MB 2.6 MB/s eta 0:00:12\n",
      "   -------------------------------- ------- 121.1/150.0 MB 2.6 MB/s eta 0:00:12\n",
      "   -------------------------------- ------- 121.6/150.0 MB 2.6 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 122.4/150.0 MB 2.6 MB/s eta 0:00:11\n",
      "   -------------------------------- ------- 123.2/150.0 MB 2.6 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 123.7/150.0 MB 2.6 MB/s eta 0:00:11\n",
      "   --------------------------------- ------ 124.5/150.0 MB 2.6 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 125.0/150.0 MB 2.6 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 125.8/150.0 MB 2.6 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 126.6/150.0 MB 2.6 MB/s eta 0:00:10\n",
      "   --------------------------------- ------ 127.1/150.0 MB 2.6 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 127.7/150.0 MB 2.6 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 128.2/150.0 MB 2.6 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 128.7/150.0 MB 2.6 MB/s eta 0:00:09\n",
      "   ---------------------------------- ----- 129.2/150.0 MB 2.6 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 129.8/150.0 MB 2.6 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 130.3/150.0 MB 2.6 MB/s eta 0:00:08\n",
      "   ---------------------------------- ----- 130.8/150.0 MB 2.6 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 131.6/150.0 MB 2.6 MB/s eta 0:00:08\n",
      "   ----------------------------------- ---- 132.1/150.0 MB 2.6 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 132.6/150.0 MB 2.6 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 133.4/150.0 MB 2.6 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 134.0/150.0 MB 2.6 MB/s eta 0:00:07\n",
      "   ----------------------------------- ---- 134.5/150.0 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 135.3/150.0 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 135.8/150.0 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 136.3/150.0 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 136.8/150.0 MB 2.6 MB/s eta 0:00:06\n",
      "   ------------------------------------ --- 137.4/150.0 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 137.9/150.0 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------------------------------ --- 138.4/150.0 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 138.9/150.0 MB 2.6 MB/s eta 0:00:05\n",
      "   ------------------------------------- -- 139.7/150.0 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 140.2/150.0 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 140.8/150.0 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 141.3/150.0 MB 2.6 MB/s eta 0:00:04\n",
      "   ------------------------------------- -- 141.8/150.0 MB 2.6 MB/s eta 0:00:04\n",
      "   -------------------------------------- - 142.6/150.0 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 143.1/150.0 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 143.9/150.0 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 144.4/150.0 MB 2.6 MB/s eta 0:00:03\n",
      "   -------------------------------------- - 145.0/150.0 MB 2.6 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 146.0/150.0 MB 2.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  146.8/150.0 MB 2.6 MB/s eta 0:00:02\n",
      "   ---------------------------------------  147.6/150.0 MB 2.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------  148.4/150.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.2/150.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------  149.9/150.0 MB 2.7 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 150.0/150.0 MB 2.5 MB/s eta 0:00:00\n",
      "Downloading lightgbm-4.6.0-py3-none-win_amd64.whl (1.5 MB)\n",
      "   ---------------------------------------- 0.0/1.5 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 0.5/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 0.5/1.5 MB 2.4 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 1.3/1.5 MB 2.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 1.5/1.5 MB 2.0 MB/s eta 0:00:00\n",
      "Installing collected packages: xgboost, lightgbm\n",
      "Successfully installed lightgbm-4.6.0 xgboost-3.0.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install xgboost lightgbm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d8385-ab02-44ac-8af8-df80c893acfd",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c0fd64-0379-4395-827e-9dc7d7775916",
   "metadata": {},
   "source": [
    "## üî∑ Step 4: Train a Basic XGBoost Classifier on the Iris Dataset\n",
    "\n",
    "### üìå Goal:\n",
    "\n",
    "Use `XGBoost` to classify **Iris flower species** based on petal/sepal length and width.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "39c601b7-3527-43ce-aceb-19b2981ae373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lucky\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [13:03:35] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from xgboost import XGBClassifier \n",
    "# xgboost: A separate library made for fast and accurate boosting models.\n",
    "# XGBClassifier: A class from xgboost used for classification tasks (like predicting Iris species).\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = load_iris() #load_iris(): Loads the dataset and returns it as a dictionary-like object.\n",
    "x = iris.data #The table of measurements ‚Äî like sepal length, petal width, etc.\n",
    "y = iris.target #iris.target: Contains numbers (0, 1, 2) representing flower species.\n",
    "# iris: Stores the loaded dataset. It has:\n",
    "# iris.data = Features (measurements)\n",
    "# iris.target = Labels (species: 0, 1, 2)\n",
    "\n",
    "# Split into train and test sets\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 42) #random_state=42: Ensures the same random split every time you run it.\n",
    "\n",
    "# Initialize XGBoost Classifier\n",
    "model = XGBClassifier(use_label_encoder=False, eval_metric = \"mlogloss\")\n",
    "# use_label_encoder=False: Prevents XGBoost from giving a warning about encoding labels.\n",
    "# eval_metric='mlogloss': Tells XGBoost to use multi-class log loss as evaluation metric ‚Äî good for multi-class classification.\n",
    "\n",
    "# Train the model\n",
    "model.fit(x_train, y_train)\n",
    "\n",
    "# Predict on test data\n",
    "y_pred = model.predict(x_test)\n",
    "\n",
    "# Calculate accuracy\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708949e7-9dcc-421f-8f02-282ac0ae907d",
   "metadata": {},
   "source": [
    "That warning :\n",
    "\n",
    "\"use_label_encoder\" are not used.\n",
    "\n",
    " üî∏ It just means newer versions of XGBoost no longer need that parameter.\n",
    "\n",
    "‚úÖ You can safely ignore it ‚Äî or even remove use_label_encoder=False altogether."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa80cb1a-9ec7-4a71-b2ba-2a805ec4e5a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
